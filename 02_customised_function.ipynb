{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111438ce-4346-4b3e-be10-c875e9f27882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39215995-1609-43b3-ae42-94958c786df4",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Nosso `script python` precisa usar a **API** do spark chamada `pyspark` para se conectar ao **Cluster do Spark**, que é onde irá acontecer toda a transformação dos dados de maneira rápida.\n",
    "\n",
    "Ou seja, para que as análises de dados sejam feitas de maneira mais rápida usando o spark, precisamos utilizar as funções ou métodos do Pyspark, que irá enviar as mensagens para os WORKERS do Spark. Contudo, as vezes temos pipelines que são muito específicos, então devemos transformar em funções python a trasnformação que queremos realizar, e aplicar o **apply** da instância do pyspark para que os dados sejam processados mais rápido no microservidor que está o Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8ae89-3182-4373-867d-604200532a14",
   "metadata": {},
   "source": [
    "### Função Customizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863efc87-ef26-4dbc-9f4a-1f529e26267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/28 22:42:32 WARN Utils: Your hostname, miguel-ubuntu2404 resolves to a loopback address: 127.0.1.1; using 192.168.0.186 instead (on interface wlp3s0)\n",
      "24/11/28 22:42:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/28 22:42:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Criar uma nova SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exemplo PySpark\") \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc23fde2-5a2f-46f2-a8ed-7df469475cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f1a560-68b8-486d-829f-1c78aba63bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/28 22:42:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 28|\n",
      "|  2|  Bob| 35|\n",
      "|  3|Cathy| 29|\n",
      "|  4|David| 40|\n",
      "+---+-----+---+\n",
      "\n",
      "DataFrame Transformado:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------------+\n",
      "| id| name|age|age_category|\n",
      "+---+-----+---+------------+\n",
      "|  1|Alice| 28|       Young|\n",
      "|  2|  Bob| 35|       Adult|\n",
      "|  3|Cathy| 29|       Young|\n",
      "|  4|David| 40|         Old|\n",
      "+---+-----+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Configurar o Spark (spark instance above - Done)\n",
    "\n",
    "# 2. Criar um DataFrame de exemplo\n",
    "data = [\n",
    "    (1, \"Alice\", 28),\n",
    "    (2, \"Bob\", 35),\n",
    "    (3, \"Cathy\", 29),\n",
    "    (4, \"David\", 40)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar o DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "df.show()\n",
    "\n",
    "# 3. Criar uma função Python personalizada\n",
    "def categorize_age(age):\n",
    "    if age < 30:\n",
    "        return \"Young\"\n",
    "    elif 30 <= age < 40:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Old\"\n",
    "\n",
    "# Converter a função Python para uma UDF (User Defined Function) do PySpark\n",
    "# P.S. No PySpark, ao registrar uma função Python como uma UDF (User Defined Function),\n",
    "# é necessário informar explicitamente o TIPO de dado que será retornado pela FUNÇÃO.\n",
    "categorize_age_udf = udf(\n",
    "    categorize_age, # função python do usuário\n",
    "    StringType() # Especificando o tipo da função\n",
    ")\n",
    "\n",
    "# Aplicar a UDF ao DataFrame\n",
    "df_transformed = df.withColumn(\n",
    "    \"age_category\",\n",
    "    categorize_age_udf(col(\"age\"))\n",
    ")\n",
    "\n",
    "# Mostrar o DataFrame transformado\n",
    "print(\"DataFrame Transformado:\")\n",
    "df_transformed.show()\n",
    "\n",
    "\n",
    "# ALWAYS FINISH A INSTANCE\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812433e-cd69-4406-9d60-87bb6fef6341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
